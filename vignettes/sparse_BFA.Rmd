---
title: "Sparse Bayesian Factor Analysis Model"
output: rmarkdown::html_vignette
bibliography: ../inst/REFERENCES.bib
vignette: >
  %\VignetteIndexEntry{Sparse Bayesian Factor Analysis Model}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

We include an implementation of Hamiltonian Monte Carlo-based sparse Bayesian factor analysis model. We observed iid $d$-variate data $Y_{1},\dots,Y_{n}$ where 
\begin{equation}
\mathbf{Y}_{i} \sim N_{d}( \mathbf{0}, \mathbf{\Sigma}=\mathbf{\Lambda}\mathbf{\Lambda}^{T}+\mathbf{\Delta} ) \text{ where } \mathbf{\Delta}=\mathrm{diag}(\delta_{1}^{2},\dots,\delta_{d}^{2}).
\end{equation} Our implementation aims to recover the marginal covariance matrix $\mathbf{\Sigma}$, and also the factor loading matrix $\mathbf{\Lambda}$ and the diagonal idiosyncratic variance matrix $\mathbf{\Delta}$ from observed $Y_{1},\dots,Y_{n}$.

We assume the following priors \begin{equation}
\mathrm{vectorize}(\mathbf{\Lambda})\sim \mathrm{DL(a)}, ~~ \log\delta_{j}^{2} \sim \mathrm{N}(\mu_{\delta},\sigma_{\delta}^{2}),
\end{equation} where $\mathrm{DL(a)}$ denotes the Dirichlet-Laplace prior [@dir_laplace] with shrinkage parameter $a$.

# Simulate Data from a sparse Bayesian factor analysis model

We sample data from the Bayesian factor analysis analysis and recover the parameters.

First, we set a seed for reprodicibility:

```{r set_seed}
set.seed(100)
```

Load `SUFA`

```{r load_SUFA}
library(SUFA)
```

```{r load_R_funcs, message=FALSE, results=FALSE, warning=FALSE, comment=FALSE}
library(here)
source(here("vignettes","simulate_data_fxns.R")) #We load some R functions
```

## Set Dimensions and Sample Sizes

We choose dimension $(d)$, sample size $(n)$, column dimension $(q)$ of $\mathbf{\Lambda}$:

```{r set_dimension}
d= 200 ## observed dimension
q= 20 ## latent dimension of the shared subspace
n=d/2
```

We choose $n=d/2$ to replicate a **large dimension small sample size** scenario.

We repeat the simulations for different structures of $\mathbf{\Lambda}$.

## Scenario 1

### Sample a Sparse Shared Loading Matrix $\mathbf{\Lambda}$

```{r gen_lambda}
val=2
  lam=simulate_lambda_sparse (k = q,p=d,pr = .25,val = val)
  blank_rows=which(rowSums(lam)==0)
  for(bl in blank_rows){
    lam[bl,sample.int(q,5)]= runif(5,-val,val)
  }
```

Then we plot the simulated $\mathbf{\Lambda}$

```{r plot_lambda, fig.dim = c(3, 3.75)}
library(ggplot2)
heatplot(lam)
```

Set the diagonal idiosyncratic variance matrix $\mathbf{\Delta}=0.5\times \mathbf{I}_{d}$

```{r idiosyncratic}
diag_sd=sqrt(.5) ##sd
```

### Sample data

```{r sample}
covmat_true=tcrossprod(lam)+ (diag_sd^2)*diag(d)
Y=mvtnorm::rmvnorm(n=n,sigma=covmat_true)
```

`Y` is a $n\times d$ `matrix`.

## Fit the SUFA Model

```{r fit_SUFA, message=FALSE, results=FALSE, warning=FALSE, comment=FALSE}
time.taken=system.time(res<-fit_FA(Y,qmax=25,nrun = 7.5e3,
                                     nleapfrog = 3, leapmax = 6, del_range = c(0,.01)))
```

**Note:** Depending on the acceptance probability, `nleapfrog` and `del_range` might need some tweaks.

We report the acceptance probability and execution time for 7,500 MCMC iterations:

```{r}
time.taken

res$acceptance_probability
```

We discard the first 2,500 iterations as the burnin samples. Since we used a thinning interval of 5 to store the MCMC samples, we set `burnin=2500/5=500`.

```{r set_burnin}
burnin=500
```

### Compare True and Estimated Covariance Matrix $\mathbf{\Sigma}=\mathbf{\Lambda}\mathbf{\Lambda}^{T}+\mathbf{\Delta}$

We consider the posterior mean of $\mathbf{\Sigma}$ as the point estimate:

```{r shared_sigma_mean}
covmat_est=SUFA_shared_covmat(res,burn = burnin)
```

We plot the true and estimated correlation matrices, i.e., `cov2cor`$(\mathbf{\Sigma})$:

```{r plot_sigmas, fig.dim = c(8, 5)}
cormat_true=cov2cor(covmat_true)
cormat_est=cov2cor(covmat_est)
sig_true=heatplot(cormat_true)+ theme(legend.position='bottom')+labs(title="True")
sig_est=heatplot(cormat_est)+ theme(legend.position='bottom')+labs(title="Estimated")
ggpubr::ggarrange(sig_true,sig_est,ncol = 2)
```

For easier visualization of the correlation structure, we now plot the above but use gray color for correlations having absolute value less then 0.10.

```{r plot_sigmas_sparse, fig.dim = c(8, 5)}
cormat_true[abs(cormat_true)<.1] =NA
cormat_est[abs(cormat_est)<.1] =NA
sig_true=heatplot(cormat_true)+ theme(legend.position='bottom')+labs(title="True")
sig_est=heatplot(cormat_est)+ theme(legend.position='bottom')+labs(title="Estimated")
ggpubr::ggarrange(sig_true,sig_est,ncol = 2)
```

```{r remove_ggplots, echo=FALSE}
rm(sig_true,sig_est,cormat_true,cormat_est)
```

### Compare True and Estimated Shared Loading Matrix $\mathbf{\Lambda}$

We use the post-processing scheme described in the paper to obtain a point estimate of $\mathbf{\Lambda}$:

```{r posterior_lambda}
est_lam=lam.est(res$Lambda,burn = burnin)
```

We plot the true and estimated $(\mathbf{\Lambda})$ with grey color to encode 0 values:

```{r plot_lambdass, fig.dim = c(6, 5)}
lam.sp=lam; lam.sp[lam==0]=NA
est_lam.sp=est_lam; est_lam.sp[est_lam==0]=NA

lam_true.gg=heatplot(lam.sp)+labs(title="True")
lam_est.gg=heatplot(est_lam.sp)+labs(title="Estimated")
ggpubr::ggarrange(lam_true.gg,lam_est.gg,ncol = 2)
```

```{r echo=F}
rm(lam.sp,est_lam.sp, lam_true.gg, lam_est.gg)
```

#### Quantifying the Recovery Level of $\mathbf{\Lambda}$

We quantify whether the post-processed $\widehat{\mathbf{\Lambda}}$ recovers true ${\mathbf{\Lambda}}$ by regressing each column of ${\mathbf{\Lambda}}$ on $\widehat{\mathbf{\Lambda}}$ and report the coefficient of determination $R^2$'s:

```{r coeff_r2}
fit_lm=summary(lm(lam~est_lam)) ## regress true lambda on estimated lambda
summary(sapply(fit_lm, function(x) x$r.squared)) ## report the r-squares r-squares
```

$R^2$ values close to 1 indicates that the true ${\mathbf{\Lambda}}$ is indeed recovered up to some non-singular matrix multiplication.

## Scenario 2

### Sample a Sparse Shared Loading Matrix $\mathbf{\Lambda}$

```{r gen_lambda_FM2}
val=2
  lam=simulate_lambda_sparse2 (k = q,p=d,pr = .5,val = val)
  blank_rows=which(rowSums(lam)==0)
  for(bl in blank_rows){
    lam[bl,sample.int(q,5)]= runif(5,-val,val)
  }
```

Then we plot the simulated $\mathbf{\Lambda}$

```{r plot_lambda_FM2, fig.dim = c(3, 3.75)}
library(ggplot2)
heatplot(lam)
```

**Note:** The above $\mathbf{\Lambda}$ is DIFFERENT from Scenario 1.

Set the diagonal idiosyncratic variance matrix $\mathbf{\Delta}=0.5\times \mathbf{I}_{d}$

```{r idiosyncratic_FM2}
diag_sd=sqrt(.5) ##sd
```

### Sample data

```{r sample_FM2}
covmat_true=tcrossprod(lam)+ (diag_sd^2)*diag(d)
Y=mvtnorm::rmvnorm(n=n,sigma=covmat_true)
```

`Y` is a $n\times d$ `matrix`.

## Fit the SUFA Model

```{r fit_SUFA_FM2, message=FALSE, results=FALSE, warning=FALSE, comment=FALSE}
time.taken=system.time(res<-fit_FA(Y,qmax=25,nrun = 7.5e3,
                                     nleapfrog = 3, leapmax = 6, del_range = c(0,.01)))
```

**Note:** Depending on the acceptance probability, `nleapfrog` and `del_range` might need some tweaks.

We report the acceptance probability and execution time for 7,500 MCMC iterations:

```{r}
time.taken

res$acceptance_probability
```

We discard the first 2,500 iterations as the burnin samples. Since we used a thinning interval of 5 to store the MCMC samples, we set `burnin=2500/5=500`.

```{r set_burnin_FM2}
burnin=500
```

### Compare True and EstimatedCovariance Matrix $\mathbf{\Sigma}=\mathbf{\Lambda}\mathbf{\Lambda}^{T}+\mathbf{\Delta}$

We consider the posterior mean of $\mathbf{\Sigma}$ as the point estimate:

```{r shared_sigma_mean_FM2}
covmat_est=SUFA_shared_covmat(res,burn = burnin)
```

We plot the true and estimated correlation matrices, i.e., `cov2cor`$(\mathbf{\Sigma})$:

```{r plot_sigmas_FM2, fig.dim = c(8, 5)}
cormat_true=cov2cor(covmat_true)
cormat_est=cov2cor(covmat_est)
sig_true=heatplot(cormat_true)+ theme(legend.position='bottom')+labs(title="True")
sig_est=heatplot(cormat_est)+ theme(legend.position='bottom')+labs(title="Estimated")
ggpubr::ggarrange(sig_true,sig_est,ncol = 2)
```

For easier visualization of the correlation structure, we now plot the above but use gray color for correlations having absolute value less then 0.10.

```{r plot_sigmas_sparse_FM2, fig.dim = c(8, 5)}
cormat_true[abs(cormat_true)<.1] =NA
cormat_est[abs(cormat_est)<.1] =NA
sig_true=heatplot(cormat_true)+ theme(legend.position='bottom')+labs(title="True")
sig_est=heatplot(cormat_est)+ theme(legend.position='bottom')+labs(title="Estimated")
ggpubr::ggarrange(sig_true,sig_est,ncol = 2)
```

```{r remove_ggplots_FM2, echo=FALSE}
rm(sig_true,sig_est,cormat_true,cormat_est)
```

### Compare True and Estimated Shared Loading Matrix $\mathbf{\Lambda}$

We use the post-processing scheme described in the paper to obtain a point estimate of $\mathbf{\Lambda}$:

```{r posterior_lambda_FM2}
est_lam=lam.est(res$Lambda,burn = burnin)
```

We plot the true and estimated $(\mathbf{\Lambda})$ with grey color to encode 0 values:

```{r plot_lambdass_FM2, fig.dim = c(6, 5)}
lam.sp=lam; lam.sp[lam==0]=NA
est_lam.sp=est_lam; est_lam.sp[est_lam==0]=NA

lam_true.gg=heatplot(lam.sp)+labs(title="True")
lam_est.gg=heatplot(est_lam.sp)+labs(title="Estimated")
ggpubr::ggarrange(lam_true.gg,lam_est.gg,ncol = 2)
```

```{r echo=F}
rm(lam.sp,est_lam.sp, lam_true.gg, lam_est.gg)
```

#### Quantifying the Recovery Level of $\mathbf{\Lambda}$

We quantify whether the post-processed $\widehat{\mathbf{\Lambda}}$ recovers true ${\mathbf{\Lambda}}$ by regressing each column of ${\mathbf{\Lambda}}$ on $\widehat{\mathbf{\Lambda}}$ and report the coefficient of determination $R^2$'s:

```{r coeff_r2_FM2}
fit_lm=summary(lm(lam~est_lam)) ## regress true lambda on estimated lambda
summary(sapply(fit_lm, function(x) x$r.squared)) ## report the r-squares r-squares
```

$R^2$ values close to 1 indicates that the true ${\mathbf{\Lambda}}$ is indeed recovered up to some non-singular matrix multiplication.

## Scenario 3

### Sample a Sparse Shared Loading Matrix $\mathbf{\Lambda}$

```{r gen_lambda_FM3}
lam=matrix(0,nrow = d, ncol = q)
  nonzero=floor(d*.15); val=2
  for(i in 1:q){
    lam[sample.int(size=nonzero,n = d) ,i]=runif(nonzero,-val,val)
  }
  blank_rows=which(rowSums(lam)==0)
  for(bl in blank_rows){
    lam[bl,sample.int(q,5)]= runif(5,-val,val)
  }
```

Then we plot the simulated $\mathbf{\Lambda}$

```{r plot_lambda_FM3, fig.dim = c(3, 3.75)}
library(ggplot2)
heatplot(lam)
```

**Note:** The above $\mathbf{\Lambda}$ is DIFFERENT from Scenarios 1 and 2.

Set the diagonal idiosyncratic variance matrix $\mathbf{\Delta}=0.5\times \mathbf{I}_{d}$

```{r idiosyncratic_FM3}
diag_sd=sqrt(.5) ##sd
```

### Sample data

```{r sample_FM3}
covmat_true=tcrossprod(lam)+ (diag_sd^2)*diag(d)
Y=mvtnorm::rmvnorm(n=n,sigma=covmat_true)
```

`Y` is a $n\times d$ `matrix`.

## Fit the SUFA Model

```{r fit_SUFA_FM3, message=FALSE, results=FALSE, warning=FALSE, comment=FALSE}
time.taken=system.time(res<-fit_FA(Y,qmax=25,nrun = 7.5e3,
                                     nleapfrog = 3, leapmax = 6, del_range = c(0,.01)))
```

**Note:** Depending on the acceptance probability, `nleapfrog` and `del_range` might need some tweaks.

We report the acceptance probability and execution time for 7,500 MCMC iterations:

```{r}
time.taken

res$acceptance_probability
```

We discard the first 2,500 iterations as the burnin samples. Since we used a thinning interval of 5 to store the MCMC samples, we set `burnin=2500/5=500`.

```{r set_burnin_FM3}
burnin=500
```

### Compare True and Estimated Covariance Matrix $\mathbf{\Sigma}=\mathbf{\Lambda}\mathbf{\Lambda}^{T}+\mathbf{\Delta}$

We consider the posterior mean of $\mathbf{\Sigma}$ as the point estimate:

```{r shared_sigma_mean_FM3}
covmat_est=SUFA_shared_covmat(res,burn = burnin)
```

We plot the true and estimated correlation matrices, i.e., `cov2cor`$(\mathbf{\Sigma})$:

```{r plot_sigmas_FM3, fig.dim = c(8, 5)}
cormat_true=cov2cor(covmat_true)
cormat_est=cov2cor(covmat_est)
sig_true=heatplot(cormat_true)+ theme(legend.position='bottom')+labs(title="True")
sig_est=heatplot(cormat_est)+ theme(legend.position='bottom')+labs(title="Estimated")
ggpubr::ggarrange(sig_true,sig_est,ncol = 2)
```

For easier visualization of the correlation structure, we now plot the above but use gray color for correlations having absolute value less then 0.10.

```{r plot_sigmas_sparse_FM3, fig.dim = c(8, 5)}
cormat_true[abs(cormat_true)<.1] =NA
cormat_est[abs(cormat_est)<.1] =NA
sig_true=heatplot(cormat_true)+ theme(legend.position='bottom')+labs(title="True")
sig_est=heatplot(cormat_est)+ theme(legend.position='bottom')+labs(title="Estimated")
ggpubr::ggarrange(sig_true,sig_est,ncol = 2)
```

```{r remove_ggplots_FM3, echo=FALSE}
rm(sig_true,sig_est,cormat_true,cormat_est)
```

### Compare True and Estimated Shared Loading Matrix $\mathbf{\Lambda}$

We use the post-processing scheme described in the paper to obtain a point estimate of $\mathbf{\Lambda}$:

```{r posterior_lambda_FM3}
est_lam=lam.est(res$Lambda,burn = burnin)
```

We plot the true and estimated $(\mathbf{\Lambda})$ with grey color to encode 0 values:

```{r plot_lambdass_FM3, fig.dim = c(6, 5)}
lam.sp=lam; lam.sp[lam==0]=NA
est_lam.sp=est_lam; est_lam.sp[est_lam==0]=NA

lam_true.gg=heatplot(lam.sp)+labs(title="True")
lam_est.gg=heatplot(est_lam.sp)+labs(title="Estimated")
ggpubr::ggarrange(lam_true.gg,lam_est.gg,ncol = 2)
```

```{r echo=F}
rm(lam.sp,est_lam.sp, lam_true.gg, lam_est.gg)
```

#### Quantifying the Recovery Level of $\mathbf{\Lambda}$

We quantify whether the post-processed $\widehat{\mathbf{\Lambda}}$ recovers true ${\mathbf{\Lambda}}$ by regressing each column of ${\mathbf{\Lambda}}$ on $\widehat{\mathbf{\Lambda}}$ and report the coefficient of determination $R^2$'s:

```{r coeff_r2_FM3}
fit_lm=summary(lm(lam~est_lam)) ## regress true lambda on estimated lambda
summary(sapply(fit_lm, function(x) x$r.squared)) ## report the r-squares r-squares
```

$R^2$ values close to 1 indicates that the true ${\mathbf{\Lambda}}$ is indeed recovered up to some non-singular matrix multiplication.

## References
